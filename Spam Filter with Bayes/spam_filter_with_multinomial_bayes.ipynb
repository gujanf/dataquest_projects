{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "Using Multinomial Naive Bayes Theorem, we utilize the assumption of conditional independence between words within an SMS message and messages already classified as spam/non-spam to decide if a new message belongs to one category or another.\n",
    "\n",
    "We have also learned to use Laplace Smoothing to correct for words that are present in the data set but not in the specific segment we are looking into (spam/non-spam).\n",
    "\n",
    "The posterior distribution in a multinomial Bayesian setting is given by:\n",
    "\n",
    "$$ P(\\theta | X) \\propto P(X | \\theta) P(\\theta) $$\n",
    "\n",
    "where:\n",
    "- theta represents the parameters (e.g., probabilities of categories in a multinomial distribution).\n",
    "- \\( X \\) represents the observed data.\n",
    "- P(X | theta) is the likelihood, which in the case of a multinomial distribution is:\n",
    "\n",
    "$$ P(X | \\theta) = \\frac{N!}{x_1! x_2! \\dots x_k!} \\theta_1^{x_1} \\theta_2^{x_2} \\dots \\theta_k^{x_k} $$\n",
    "\n",
    "We are now going to use this theory and apply it to a practical example, where we will analyze a [real data set](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection) put together by Tiago A. Almeida and José María Gómez Hidalgo from UC Irvine. We'll use the multinomial Naive Bayes algorithm along with the dataset's 5,572 SMS messages that are already classified by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "% matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# read the csv file and use tabs as separators\n",
    "sms = pd.read_csv('SMSSpamCollection', \n",
    "                  sep ='\\t', \n",
    "                  header = None, \n",
    "                  names = ['Label', 'SMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "Label    5572 non-null object\n",
      "SMS      5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "sms.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                     SMS\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the 'Label' column we have two distinct values: \n",
    "- spam: messages classified as spam\n",
    "- ham: messages classified as non-spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['Label'].value_counts(normalize = True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dup = sms[sms['SMS'].duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>403</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>309</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label                     SMS\n",
       "count    403                     403\n",
       "unique     2                     281\n",
       "top      ham  Sorry, I'll call later\n",
       "freq     309                      29"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     76.674938\n",
       "spam    23.325062\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup['Label'].value_counts(normalize = True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we know that we have:\n",
    "- 5572 messages, of which 5169 have a distinct value\n",
    "    - 76.7% of those duplicated messages are non-spam\n",
    "- 86.6% of those are non-spam, 13.4% are spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Set\n",
    "\n",
    "Before coming up with the algorithm, we need to make sure that we will be able to properly test it. For that purpose, once our filter is done we are going to split our dataset in two:\n",
    "\n",
    "- A training set to 'train' the computer on message classification\n",
    "- A test set, which we will use to compare our algorithm's output to the spam label that already exists\n",
    "\n",
    "To make sure that we have enough data for both tasks, we will use 80% of messages for training (4,458 messages), and the other 20% for testing (1,114 messages). \n",
    "\n",
    "Our goal for this project is to obtain a spam filter that classifies new messages as spam/non-spam. Therefore, if we can show that the label we will create for the 1,114 'new' messages matches 80%+ of the pre-existing, human-created labels, then this project will be considered a success.\n",
    "\n",
    "We will first start by randomizing our entire dataset, using random state = 1 to make sure our results can be reproduced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomize entire dataset\n",
    "sms_randomized = sms.sample(frac=1, random_state=1).reset_index(drop = True)\n",
    "\n",
    "# split dataset into training and test sets\n",
    "train = sms_randomized.loc[0:4458, :].reset_index(drop = True).copy()\n",
    "test = sms_randomized.loc[4458:, :].reset_index(drop = True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we want to make sure that the spam to non-spam ration is similar in both sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     86.544068\n",
      "spam    13.455932\n",
      "Name: Label, dtype: float64 \n",
      " ham     86.804309\n",
      "spam    13.195691\n",
      "Name: Label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(train['Label'].value_counts(normalize=True)*100, \n",
    "      '\\n',\n",
    "      test['Label'].value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratios are very similar, so we can continue with the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Case and Punctuation\n",
    "\n",
    "With out dataset split, we now have to use our training set to teach the algorithm to classify new messages.\n",
    "\n",
    "To do so, we will continue to use the Multinomial Naive Algorithm so that:\n",
    "\n",
    "$$\n",
    "P(Spam | w_1,w_2,...,w_n)\\ \\alpha\\ P(Spam) * \\prod_{i=1}^n P(w_i | Spam)\n",
    "$$\n",
    "$$\n",
    "P(Ham | w_1,w_2,...,w_n)\\ \\alpha\\ P(Ham) * \\prod_{i=1}^n P(w_i | Ham)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i | Spam} + \\alpha}{N_{Spam} + \\alpha * N_{Vocabulary}}\n",
    "$$\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i | Ham} + \\alpha}{N_{Ham} + \\alpha * N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "In the equations above we have various terms:\n",
    "\n",
    "$$ N_{w_i | Spam} =  \\mbox{the  number  of  times  the  word w_i occurs in spam messages} $$\n",
    "$$ N_{w_i | Spam^C} =  \\mbox{the  number  of  times  the  word w_i occurs in non-spam messages} $$\n",
    "\n",
    "$$ N_{w_i | Spam} =  \\mbox{total number of words in spam messages} $$\n",
    "$$ N_{w_i | Spam^C} =  \\mbox{total number of words in non-spam messages} $$\n",
    "\n",
    "$$ N_{Vocabulary} =  \\mbox{total number of words in the vocabulary} $$\n",
    "$$ \\alpha =  \\mbox{1 (alpha is a smoothing parameter)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before designing the algorithm, we need to do some data cleaning. To have messages that we can properly analyze, we need to remove everything that is not a character from a-z, A-Z or 0-9, and we will also want to convert everything to lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.loc[:,'SMS'] = train['SMS'].apply(lambda x: re.sub('\\W', ' ', x))\n",
    "train.loc[:,'SMS'] = train['SMS'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary\n",
    "\n",
    "We call the set of each individual words in our SMS column our 'vocabulary'. We need this set because eventually we will have an individual column for each unique word, and we need to create a list with these before that is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = [] # list with individual words\n",
    "\n",
    "# create new column with messages split in a list\n",
    "train['SMS_SPLIT'] = train['SMS'].apply(lambda x: x.split())\n",
    "# iterate through list of words and append new words to vocabulary list\n",
    "train['SMS_SPLIT'].apply(lambda x: [vocab.append(i) for i in x if i not in vocab])\n",
    "\n",
    "# transform vocab list into a set to remove duplicates and then back into a list\n",
    "vocab = set(vocab)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalizing Training Set\n",
    "\n",
    "Now that we have our vocabulary list, we need to create a dictionary with the frequency count of each individual word. This will then be used to create the final version of our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize dictionary with 0s in all rows, with as many rows as messages in the training dataset\n",
    "# Each column will be an individual word from the vocabulary\n",
    "word_counts_per_sms = {key : [0] * len(train['SMS_SPLIT']) for key in vocab}\n",
    "\n",
    "# loop through messages while using enumerate to get both index and the SMS message\n",
    "for index, sms in enumerate(train['SMS_SPLIT']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the code above, we should now have a dictionary where each key is a distinct word from the SMS messages. Each of those keys should have a list as long as the dataset as value. Within said list, the index corresponds to a message in the dataset with the same index, and the value corresponds to the number of times that the key appears in said message. \n",
    "\n",
    "Now the last step to get the training set ready is converting this dictionary to a data frame, and concatenating this data frame to our original training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>SMS_SPLIT</th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7788 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  \\\n",
       "0   ham                       yep  by the pretty sculpture   \n",
       "1   ham      yes  princess  are you going to make me moan    \n",
       "2   ham                         welp apparently he retired   \n",
       "3   ham                                            havent    \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ...   \n",
       "\n",
       "                                           SMS_SPLIT  0  00  000  000pes  \\\n",
       "0                  [yep, by, the, pretty, sculpture]  0   0    0       0   \n",
       "1  [yes, princess, are, you, going, to, make, me,...  0   0    0       0   \n",
       "2                    [welp, apparently, he, retired]  0   0    0       0   \n",
       "3                                           [havent]  0   0    0       0   \n",
       "4  [i, forgot, 2, ask, ü, all, smth, there, s, a,...  0   0    0       0   \n",
       "\n",
       "   008704050406  0089  01223585334 ...  zindgi  zoe  zogtorius  zouk  zyada  \\\n",
       "0             0     0            0 ...       0    0          0     0      0   \n",
       "1             0     0            0 ...       0    0          0     0      0   \n",
       "2             0     0            0 ...       0    0          0     0      0   \n",
       "3             0     0            0 ...       0    0          0     0      0   \n",
       "4             0     0            0 ...       0    0          0     0      0   \n",
       "\n",
       "   é  ú1  ü  〨ud  鈥  \n",
       "0  0   0  0    0  0  \n",
       "1  0   0  0    0  0  \n",
       "2  0   0  0    0  0  \n",
       "3  0   0  0    0  0  \n",
       "4  0   0  2    0  0  \n",
       "\n",
       "[5 rows x 7788 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols = pd.DataFrame(word_counts_per_sms)\n",
    "clean_train = pd.concat([train,new_cols], axis = 1)\n",
    "clean_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Constants First\n",
    "\n",
    "Now that we have the clean training data, we should recall that the formulas for the Naive Bayesian model are such:\n",
    "\n",
    "$$\n",
    "P(Spam | w_1,w_2,...,w_n)\\ \\alpha\\ P(Spam) * \\prod_{i=1}^n P(w_i | Spam)\n",
    "$$\n",
    "$$\n",
    "P(Ham | w_1,w_2,...,w_n)\\ \\alpha\\ P(Ham) * \\prod_{i=1}^n P(w_i | Ham)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i | Spam} + \\alpha}{N_{Spam} + \\alpha * N_{Vocabulary}}\n",
    "$$\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i | Ham} + \\alpha}{N_{Ham} + \\alpha * N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "Within those, some values never change. We can calculate the values for:\n",
    "\n",
    "- P(Spam)\n",
    "- P(Ham)\n",
    "- $ N_{Spam}, N_{Ham}, N_{Vocabulary} $\n",
    "\n",
    "Refer to the 'Letter Case and Punctuation' section for the meaning of each of these constants. Also, for this exercise we will use a Laplace constant of $ \\alpha = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initiating all constants\n",
    "\n",
    "spam = clean_train[clean_train['Label'] == 'spam']\n",
    "ham = clean_train[clean_train['Label'] == 'ham']\n",
    "\n",
    "p_spam = len(spam) / len(clean_train) # probability of message being spam\n",
    "p_ham = len(ham) / len(clean_train) # probability of message being non-spam\n",
    "\n",
    "# create series with number of words, then sum for total words per category\n",
    "n_spam = spam['SMS_SPLIT'].apply(len).sum() # words in spam\n",
    "n_ham = ham['SMS_SPLIT'].apply(len).sum() # words in ham\n",
    "\n",
    "n_vocabulary = len(vocab) # words in vocabulary\n",
    "\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "The constants above are 'constant' because their values do not change regardless of what word we are looking into. However, $P(w_i|Spam)$ and $P(w_i|Ham)$ do change, since the probability of each word changes according to whether the message is spam or not.\n",
    "\n",
    "We will need to calculate:\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i | Spam} + \\alpha}{N_{Spam} + \\alpha * N_{Vocabulary}}\n",
    "$$\n",
    "$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i | Ham} + \\alpha}{N_{Ham} + \\alpha * N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "The steps we take to calculate P(\"secret\"|Spam) will be identical for both of our new messages above, or for any other new message that contains the word \"secret\". The key detail here is that calculating P(\"secret\"|Spam) only depends on the training set, and as long as we don't make changes to the training set, P(\"secret\"|Spam) stays constant. The same reasoning also applies to P(\"secret\"|Ham).\n",
    "\n",
    "This means that we can use our training set to calculate the probability for each word in our vocabulary. If our vocabulary contained only the words \"lost\", \"navigate\", and \"sea\", then we'd need to calculate six probabilities:\n",
    "\n",
    "- P(\"lost\"|Spam) and P(\"lost\"|Ham)\n",
    "- P(\"navigate\"|Spam) and P(\"navigate\"|Ham)\n",
    "- P(\"sea\"|Spam) and P(\"sea\"|Ham)\n",
    "\n",
    "We have 7,783 words in our vocabulary, which means we'll need to calculate a total of 15,566 probabilities. For each word, we need to calculate both $P(w_i|Spam)$ and $P(w_i|Ham)$.\n",
    "\n",
    "In more technical language, the probability values that P(wi|Spam) and P(wi|Ham) will take are called __parameters__. \n",
    "\n",
    "The fact that we calculate so many values before even beginning the classification of new messages makes the Naive Bayes algorithm very fast (especially compared to other algorithms). When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message.\n",
    "\n",
    "Recall that P(wi|Spam) and P(wi|Ham) are key parts of the equations that we need to classify the new messages:\n",
    "\n",
    "$$\n",
    "P(Spam | w_1,w_2,...,w_n)\\ \\alpha\\ P(Spam) * \\prod_{i=1}^n P(w_i | Spam)\n",
    "$$\n",
    "$$\n",
    "P(Ham | w_1,w_2,...,w_n)\\ \\alpha\\ P(Ham) * \\prod_{i=1}^n P(w_i | Ham)\n",
    "$$\n",
    "\n",
    "To calculate all of these we will initialize two dictionaries, one for spam and one for ham, where the keys are the individual words and their values are the the probabilities of said word appearing within their classification. Recall that we already formed a column for each word, so we just need to sum each column to check how many times said word appears in both spam and ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating dictionaries with SPAM and HAM parameters for entire vocabulary\n",
    "spam_parameters = {distinct_word : 0 for distinct_word in vocab}\n",
    "ham_parameters = {distinct_word : 0 for distinct_word in vocab}\n",
    "\n",
    "# we have dataframes with SPAM and HAM, need probability of finding each word in each of those dataframes\n",
    "for distinct_word in vocab:\n",
    "    \n",
    "    # number of times word happens in spam and ham will be the sum of the word's respective column\n",
    "    n_word_spam = spam[distinct_word].sum() # times word appears in spam\n",
    "    n_word_ham = ham[distinct_word].sum() # times word appears in non-spam\n",
    "    \n",
    "    # use these along with constants to get the probability of a word appearing in either group\n",
    "    prob_word_given_spam = (n_word_spam + alpha) / (n_spam + alpha * n_vocabulary)\n",
    "    prob_word_given_ham =  (n_word_ham + alpha) / (n_ham + alpha * n_vocabulary)\n",
    "    \n",
    "    # update parameters to their respective dictionary\n",
    "    spam_parameters[distinct_word] = prob_word_given_spam\n",
    "    ham_parameters[distinct_word] = prob_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a New Message\n",
    "\n",
    "Now that we have all the needed constants and parameters we can progress to creating the spam filter. This function:\n",
    "\n",
    "- Takes in a new message as input $ (w_1, w_2,..., w_n) $\n",
    "- Calculates $ P(Spam | (w_1, w_2,..., w_n) $ and $ P(Ham | (w_1, w_2,..., w_n) $\n",
    "- Compares both values, and:\n",
    "    - If $ P(Spam | (w_1, w_2,..., w_n) > P(Ham | (w_1, w_2,..., w_n)$ then the message is classified as spam\n",
    "    - If $ P(Spam | (w_1, w_2,..., w_n) < P(Ham | (w_1, w_2,..., w_n)$ then the message is classified as non-spam\n",
    "    - If $ P(Spam | (w_1, w_2,..., w_n) = P(Ham | (w_1, w_2,..., w_n)$ then the algorithm may ask for human help\n",
    "\n",
    "We will use the functions for $ P(Spam | (w_1, w_2,..., w_n) $ and $ P(Ham | (w_1, w_2,..., w_n) $ that we displayed in the last section to develop our function to classify messages below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    \n",
    "    # format new message to match current standards\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    # prob of message being spam is a product of the prob of a message being spam times the prob of each word being found in spam messages\n",
    "    # spam/ham base probabilities will be used as a base value in case none of the words in the vocabulary are found in the message\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "    \n",
    "    for word in message:\n",
    "        if word in spam_parameters:\n",
    "            p_spam_given_message *= spam_parameters[word]\n",
    "        if word in ham_parameters:\n",
    "            p_ham_given_message *= ham_parameters[word]        \n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'Human requested'      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test an obviously spam message and one that sounds normal below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"Sounds good, Tom, then see u there\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "Now that our function seems to be working, we will create a new column in our testing dataset with the results of our algorithm's prediction. If we compare this preduction to the real values (recall that all of our sample messages were already classified as spam/non-spam by humans before), then we can determine how accurate said algorithm is for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create new column in testing dataset using the classify() function\n",
    "test['prediction'] = test['SMS'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of messages needing human intervention is  1\n"
     ]
    }
   ],
   "source": [
    "# check if any message needs human intervention\n",
    "human_needed_count = len(test[test['prediction'] == 'Human requested'])\n",
    "print('The number of messages needing human intervention is ', human_needed_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps predictably due to the number of decimals involved, only one message has been marked as needing a human. Now we can test the accuracy of our algorithm by comparing how often our prediction was the same to the existing label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1114 messages, our algorithm guesses 1100 right. This means our algorithm filtered out spam with 98.74% accuracy!\n"
     ]
    }
   ],
   "source": [
    "# bool mask for correct results\n",
    "correct_prediction_mask = test['Label'] == test['prediction']\n",
    "\n",
    "# number of correct guesses\n",
    "correct_prediction_rows = len(test[correct_prediction_mask])\n",
    "\n",
    "# number of total test messages\n",
    "total_messages = len(test)\n",
    "\n",
    "# % of correct guesses\n",
    "\n",
    "accuracy_percent = round((correct_prediction_rows / total_messages) *100, 2)\n",
    "\n",
    "print('Out of {} messages, our algorithm guesses {} right. This means our algorithm filtered out spam with {}% accuracy!'.format(total_messages, correct_prediction_rows, accuracy_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this exercise we built a spam filter for SMS messages using the Multinomial Naive Bayes algorithm. Our initial objective was to obtain at least 80% of accuracy, but we managed to do a lot better than that at 98.74% accuracy.\n",
    "\n",
    "Some of the potential next steps we could take from here are:\n",
    "\n",
    "- Checking what went wrong with the 14 messages that were classifies incorrectly\n",
    "- Making the filtering more complex by making the algorithm case sensitive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
